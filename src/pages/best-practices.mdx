---
title: "Best Practices and Common Pitfalls"
description: "Learn essential best practices for probabilistic programming and avoid common mistakes in probability applications"
---

import Prose from '../components/ui/Prose.astro';
import Callout from '../components/ui/Callout.astro';
import Pager from '../components/ui/Pager.astro';
import OnThisPage from '../components/ui/OnThisPage.astro';

<Prose>

# Best Practices and Common Pitfalls

This chapter covers essential best practices for probabilistic programming and highlights common mistakes to avoid when working with probability in practice.

<Callout variant="note">
**Learning Objectives**

By the end of this chapter, you will:
- Understand reproducibility requirements for probabilistic code
- Know how to validate probabilistic models effectively
- Recognize and avoid common statistical pitfalls
- Implement robust error handling and performance optimization
</Callout>

## Reproducibility and Random Seed Management

Reproducible results are crucial for scientific computing and debugging probabilistic code.

```python
import numpy as np
import random
import os
from datetime import datetime

# Proper seed management
def set_all_seeds(seed=42):
    """Set seeds for all random number generators"""
    np.random.seed(seed)
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"All random seeds set to: {seed}")

# Example: Reproducible Monte Carlo
def monte_carlo_pi(n_samples, seed=None):
    """Estimate π using Monte Carlo with reproducible results"""
    if seed is not None:
        set_all_seeds(seed)
    
    # Generate random points
    points = np.random.uniform(-1, 1, (n_samples, 2))
    inside_circle = np.sum(points[:, 0]**2 + points[:, 1]**2 <= 1)
    
    return 4 * inside_circle / n_samples

# Test reproducibility
pi_est1 = monte_carlo_pi(100000, seed=42)
pi_est2 = monte_carlo_pi(100000, seed=42)

print(f"First estimate: {pi_est1:.6f}")
print(f"Second estimate: {pi_est2:.6f}")
print(f"Estimates match: {pi_est1 == pi_est2}")
```

## Model Validation and Testing

Proper validation ensures that probabilistic models behave as expected.

```python
from scipy import stats

def validate_random_generator(samples, expected_dist='normal', alpha=0.05):
    """Validate that samples follow expected distribution"""
    
    if expected_dist == 'normal':
        # Shapiro-Wilk test for normality
        stat, p_value = stats.shapiro(samples[:5000])  # Limit sample size
        test_name = "Shapiro-Wilk"
    elif expected_dist == 'uniform':
        # Kolmogorov-Smirnov test for uniformity
        stat, p_value = stats.kstest(samples, 'uniform')
        test_name = "Kolmogorov-Smirnov"
    else:
        raise ValueError(f"Unsupported distribution: {expected_dist}")
    
    is_valid = p_value > alpha
    
    print(f"{test_name} Test Results:")
    print(f"Statistic: {stat:.6f}")
    print(f"P-value: {p_value:.6f}")
    print(f"Distribution valid (α={alpha}): {is_valid}")
    
    return is_valid, p_value

# Example validation
np.random.seed(42)
normal_samples = np.random.normal(0, 1, 1000)
uniform_samples = np.random.uniform(0, 1, 1000)

print("Validating Normal Distribution:")
validate_random_generator(normal_samples, 'normal')

print("\nValidating Uniform Distribution:")
validate_random_generator(uniform_samples, 'uniform')
```

## Common Statistical Pitfalls

### Multiple Testing Problem

```python
def demonstrate_multiple_testing():
    """Show the multiple testing problem and corrections"""
    
    np.random.seed(42)
    n_tests = 50
    alpha = 0.05
    
    # Perform multiple t-tests on null data
    p_values = []
    for i in range(n_tests):
        # Two samples from same distribution (null hypothesis true)
        group1 = np.random.normal(0, 1, 30)
        group2 = np.random.normal(0, 1, 30)
        
        _, p_val = stats.ttest_ind(group1, group2)
        p_values.append(p_val)
    
    p_values = np.array(p_values)
    
    # Count significant results
    significant_uncorrected = np.sum(p_values < alpha)
    
    # Bonferroni correction
    bonferroni_alpha = alpha / n_tests
    significant_bonferroni = np.sum(p_values < bonferroni_alpha)
    
    print("Multiple Testing Problem:")
    print(f"Number of tests: {n_tests}")
    print(f"Expected false positives: {n_tests * alpha:.1f}")
    print(f"Actual significant (uncorrected): {significant_uncorrected}")
    print(f"Significant after Bonferroni: {significant_bonferroni}")
    
    return p_values

p_vals = demonstrate_multiple_testing()
```

### Simpson's Paradox

```python
def demonstrate_simpsons_paradox():
    """Show Simpson's Paradox with treatment effectiveness"""
    
    # Hospital A: severe cases
    hospital_a_control_success = 20  # out of 100
    hospital_a_treatment_success = 30  # out of 100
    
    # Hospital B: mild cases  
    hospital_b_control_success = 16  # out of 20
    hospital_b_treatment_success = 17  # out of 20
    
    # Individual hospital rates
    a_control_rate = hospital_a_control_success / 100
    a_treatment_rate = hospital_a_treatment_success / 100
    b_control_rate = hospital_b_control_success / 20
    b_treatment_rate = hospital_b_treatment_success / 20
    
    # Combined rates
    combined_control_success = hospital_a_control_success + hospital_b_control_success
    combined_treatment_success = hospital_a_treatment_success + hospital_b_treatment_success
    combined_control_total = 100 + 20
    combined_treatment_total = 100 + 20
    
    combined_control_rate = combined_control_success / combined_control_total
    combined_treatment_rate = combined_treatment_success / combined_treatment_total
    
    print("Simpson's Paradox Example:")
    print(f"Hospital A - Control: {a_control_rate:.1%}, Treatment: {a_treatment_rate:.1%}")
    print(f"Hospital B - Control: {b_control_rate:.1%}, Treatment: {b_treatment_rate:.1%}")
    print(f"Combined - Control: {combined_control_rate:.1%}, Treatment: {combined_treatment_rate:.1%}")
    print("\nTreatment is better in each hospital, but worse overall!")

demonstrate_simpsons_paradox()
```

## Performance Optimization

### Efficient Sampling

```python
import time

def compare_sampling_performance():
    """Compare different sampling approaches"""
    
    n_samples = 1000000
    
    # Method 1: NumPy vectorized (recommended)
    start_time = time.time()
    samples_vectorized = np.random.normal(0, 1, n_samples)
    vectorized_time = time.time() - start_time
    
    # Method 2: Python loop (slow - for comparison)
    start_time = time.time()
    samples_loop = []
    for _ in range(min(n_samples, 10000)):  # Limit for speed
        samples_loop.append(np.random.normal())
    loop_time = time.time() - start_time
    
    print("Sampling Performance Comparison:")
    print(f"Vectorized: {vectorized_time:.4f}s for {n_samples:,} samples")
    print(f"Python loop: {loop_time:.4f}s for {min(n_samples, 10000):,} samples")
    print(f"Speedup factor: ~{(loop_time * n_samples / 10000) / vectorized_time:.0f}x")
    
    return samples_vectorized

samples = compare_sampling_performance()
```

## Error Handling Best Practices

```python
def robust_probability_calculation(data, method='mle'):
    """Robust probability calculation with error handling"""
    
    try:
        # Input validation
        if len(data) == 0:
            raise ValueError("Data cannot be empty")
        
        if not np.all(np.isfinite(data)):
            print("Warning: Non-finite values detected, removing them")
            data = data[np.isfinite(data)]
        
        # Parameter estimation
        if method == 'mle':
            mean_est = np.mean(data)
            std_est = np.std(data, ddof=1)
        else:
            raise ValueError(f"Unknown method: {method}")
        
        # Validate results
        if std_est <= 0:
            raise ValueError("Standard deviation must be positive")
        
        # Calculate log-likelihood
        log_likelihood = np.sum(stats.norm.logpdf(data, mean_est, std_est))
        
        return {
            'mean': mean_est,
            'std': std_est,
            'log_likelihood': log_likelihood,
            'n_samples': len(data),
            'success': True
        }
        
    except Exception as e:
        print(f"Error in probability calculation: {e}")
        return {
            'mean': None,
            'std': None,
            'log_likelihood': None,
            'n_samples': len(data) if 'data' in locals() else 0,
            'success': False,
            'error': str(e)
        }

# Test robust calculation
np.random.seed(42)
good_data = np.random.normal(5, 2, 100)
bad_data = np.array([1, 2, np.inf, 4, np.nan])

print("Good data results:")
result1 = robust_probability_calculation(good_data)
print(f"Mean: {result1['mean']:.3f}, Std: {result1['std']:.3f}")

print("\nBad data results:")
result2 = robust_probability_calculation(bad_data)
print(f"Success: {result2['success']}, Error: {result2.get('error', 'None')}")
```

## Key Takeaways

<Callout variant="tip">
**Essential Best Practices**

1. **Always set random seeds** for reproducible results
2. **Validate your models** using appropriate statistical tests
3. **Be aware of multiple testing** and apply corrections when needed
4. **Watch for Simpson's Paradox** when aggregating data
5. **Use vectorized operations** for better performance
6. **Implement robust error handling** for production code
7. **Document assumptions** and limitations clearly
</Callout>

<Callout variant="warning">
**Common Pitfalls to Avoid**

- Assuming independence without verification
- Ignoring multiple testing corrections
- Confusing correlation with causation
- Using inappropriate statistical tests
- Neglecting to validate model assumptions
- Poor random seed management
- Inefficient sampling methods
</Callout>

## Checklist for Probabilistic Projects

**Before Starting:**
- [ ] Define clear objectives and success criteria
- [ ] Identify assumptions and document them
- [ ] Plan for reproducibility (seeds, versions)

**During Development:**
- [ ] Validate models with appropriate tests
- [ ] Use efficient, vectorized operations
- [ ] Implement proper error handling
- [ ] Test edge cases and boundary conditions

**Before Deployment:**
- [ ] Verify reproducibility across environments
- [ ] Document all assumptions and limitations
- [ ] Provide uncertainty quantification
- [ ] Include model validation results

<Pager 
  prev={{ title: "Advanced Topics", href: "/advanced-topics" }}
  next={{ title: "Conclusion", href: "/conclusion" }}
/>

</Prose>

<OnThisPage />
