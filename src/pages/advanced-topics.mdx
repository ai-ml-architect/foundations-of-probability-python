---
title: "Advanced Topics and Modern Applications"
description: "Explore cutting-edge probability applications including machine learning, AI, and modern computational methods"
---

import Prose from '../components/ui/Prose.astro';
import Callout from '../components/ui/Callout.astro';
import Pager from '../components/ui/Pager.astro';
import OnThisPage from '../components/ui/OnThisPage.astro';

<Prose>

# Advanced Topics and Modern Applications

This chapter explores cutting-edge applications of probability theory in modern computing, artificial intelligence, and emerging technologies.

<Callout variant="note">
**Learning Objectives**

By the end of this chapter, you will:
- Understand probabilistic machine learning concepts
- Explore uncertainty quantification in deep learning
- Learn about stochastic processes and their applications
- Discover emerging trends in probabilistic computing
</Callout>

## Probabilistic Machine Learning

Modern machine learning increasingly embraces uncertainty quantification, moving beyond point estimates to full probability distributions.

### Gaussian Processes

Gaussian Processes provide a Bayesian approach to regression and classification with built-in uncertainty quantification.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import cdist
from scipy.linalg import cholesky, solve_triangular

class GaussianProcess:
    def __init__(self, kernel_func, noise_var=1e-6):
        self.kernel_func = kernel_func
        self.noise_var = noise_var
        self.X_train = None
        self.y_train = None
        self.K_inv = None
    
    def fit(self, X, y):
        self.X_train = X
        self.y_train = y
        
        # Compute kernel matrix
        K = self.kernel_func(X, X) + self.noise_var * np.eye(len(X))
        
        # Cholesky decomposition for numerical stability
        self.L = cholesky(K, lower=True)
        self.alpha = solve_triangular(self.L, y, lower=True)
        self.alpha = solve_triangular(self.L.T, self.alpha, lower=False)
    
    def predict(self, X_test, return_std=True):
        # Kernel between test and training points
        K_star = self.kernel_func(self.X_train, X_test)
        
        # Mean prediction
        mean = K_star.T @ self.alpha
        
        if return_std:
            # Variance prediction
            K_star_star = self.kernel_func(X_test, X_test)
            v = solve_triangular(self.L, K_star, lower=True)
            var = np.diag(K_star_star) - np.sum(v**2, axis=0)
            std = np.sqrt(np.maximum(var, 0))
            return mean, std
        
        return mean

def rbf_kernel(X1, X2, length_scale=1.0, signal_var=1.0):
    """Radial Basis Function (RBF) kernel"""
    distances = cdist(X1, X2, 'euclidean')
    return signal_var * np.exp(-0.5 * (distances / length_scale)**2)

# Generate synthetic data
np.random.seed(42)
n_train = 20
X_train = np.random.uniform(-3, 3, n_train).reshape(-1, 1)
y_train = np.sin(X_train.ravel()) + 0.1 * np.random.normal(0, 1, n_train)

# Test points
X_test = np.linspace(-4, 4, 100).reshape(-1, 1)
y_true = np.sin(X_test.ravel())

# Fit Gaussian Process
gp = GaussianProcess(lambda x1, x2: rbf_kernel(x1, x2, length_scale=1.0))
gp.fit(X_train, y_train)

# Predictions
y_pred, y_std = gp.predict(X_test)

print("Gaussian Process Regression:")
print(f"Training points: {n_train}")
print(f"Test points: {len(X_test)}")
print(f"Mean absolute error: {np.mean(np.abs(y_pred - y_true)):.4f}")

# Visualization
plt.figure(figsize=(12, 8))

# GP predictions
plt.subplot(2, 2, 1)
plt.plot(X_test, y_true, 'g--', label='True function', linewidth=2)
plt.plot(X_test, y_pred, 'b-', label='GP mean', linewidth=2)
plt.fill_between(X_test.ravel(), 
                 y_pred - 2*y_std, 
                 y_pred + 2*y_std, 
                 alpha=0.3, label='95% confidence')
plt.scatter(X_train, y_train, c='red', s=50, zorder=5, label='Training data')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Gaussian Process Regression')
plt.legend()
plt.grid(True, alpha=0.3)

# Uncertainty quantification
plt.subplot(2, 2, 2)
plt.plot(X_test, y_std, 'purple', linewidth=2)
plt.xlabel('x')
plt.ylabel('Prediction Std')
plt.title('Uncertainty Quantification')
plt.grid(True, alpha=0.3)

# Sample functions from GP prior
plt.subplot(2, 2, 3)
X_prior = np.linspace(-4, 4, 50).reshape(-1, 1)
K_prior = rbf_kernel(X_prior, X_prior) + 1e-6 * np.eye(len(X_prior))
L_prior = cholesky(K_prior, lower=True)

for i in range(5):
    sample = L_prior @ np.random.normal(0, 1, len(X_prior))
    plt.plot(X_prior, sample, alpha=0.7)

plt.xlabel('x')
plt.ylabel('y')
plt.title('Samples from GP Prior')
plt.grid(True, alpha=0.3)

# Posterior samples
plt.subplot(2, 2, 4)
# Generate posterior samples
n_samples = 5
posterior_samples = []

for _ in range(n_samples):
    # Sample from posterior predictive distribution
    sample = y_pred + y_std * np.random.normal(0, 1, len(X_test))
    posterior_samples.append(sample)
    plt.plot(X_test, sample, alpha=0.7)

plt.plot(X_test, y_true, 'g--', label='True function', linewidth=2)
plt.scatter(X_train, y_train, c='red', s=50, zorder=5)
plt.xlabel('x')
plt.ylabel('y')
plt.title('Samples from GP Posterior')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Variational Inference

Variational inference provides scalable approximate Bayesian inference for complex models.

```python
# Variational Bayesian Linear Regression
class VariationalLinearRegression:
    def __init__(self, alpha_prior=1.0, beta_prior=1.0):
        self.alpha_prior = alpha_prior  # Prior precision for weights
        self.beta_prior = beta_prior    # Prior precision for noise
        
    def fit(self, X, y, max_iter=100, tol=1e-6):
        n, d = X.shape
        
        # Add bias term
        X_aug = np.column_stack([np.ones(n), X])
        d_aug = d + 1
        
        # Initialize variational parameters
        # For weights: q(w) = N(mu_w, Sigma_w)
        mu_w = np.zeros(d_aug)
        Sigma_w = np.eye(d_aug)
        
        # For noise precision: q(beta) = Gamma(a_beta, b_beta)
        a_beta = self.beta_prior + n/2
        b_beta = self.beta_prior
        
        # For weight precision: q(alpha) = Gamma(a_alpha, b_alpha)
        a_alpha = self.alpha_prior + d_aug/2
        b_alpha = self.alpha_prior
        
        prev_elbo = -np.inf
        
        for iteration in range(max_iter):
            # Update weight distribution
            E_beta = a_beta / b_beta
            Sigma_w_inv = E_beta * (X_aug.T @ X_aug) + (a_alpha/b_alpha) * np.eye(d_aug)
            Sigma_w = np.linalg.inv(Sigma_w_inv)
            mu_w = E_beta * Sigma_w @ (X_aug.T @ y)
            
            # Update noise precision
            residual = y - X_aug @ mu_w
            b_beta = (self.beta_prior + 
                     0.5 * (np.sum(residual**2) + 
                           np.trace(X_aug.T @ X_aug @ Sigma_w)))
            
            # Update weight precision
            b_alpha = (self.alpha_prior + 
                      0.5 * (mu_w.T @ mu_w + np.trace(Sigma_w)))
            
            # Compute ELBO (Evidence Lower BOund)
            elbo = self._compute_elbo(X_aug, y, mu_w, Sigma_w, 
                                    a_alpha, b_alpha, a_beta, b_beta)
            
            if abs(elbo - prev_elbo) < tol:
                break
            prev_elbo = elbo
        
        self.mu_w = mu_w
        self.Sigma_w = Sigma_w
        self.a_beta = a_beta
        self.b_beta = b_beta
        self.converged = iteration < max_iter - 1
        
    def _compute_elbo(self, X, y, mu_w, Sigma_w, a_alpha, b_alpha, a_beta, b_beta):
        """Compute Evidence Lower BOund"""
        n, d = X.shape
        
        # Expected log likelihood
        E_beta = a_beta / b_beta
        residual = y - X @ mu_w
        exp_log_lik = (n/2 * (np.log(E_beta) - np.log(2*np.pi)) - 
                      E_beta/2 * (np.sum(residual**2) + np.trace(X.T @ X @ Sigma_w)))
        
        # KL divergences (simplified)
        kl_w = 0.5 * np.trace(Sigma_w) + 0.5 * mu_w.T @ mu_w - 0.5 * d
        kl_alpha = (a_alpha - self.alpha_prior) - a_alpha + self.alpha_prior * np.log(b_alpha)
        kl_beta = (a_beta - self.beta_prior) - a_beta + self.beta_prior * np.log(b_beta)
        
        return exp_log_lik - kl_w - kl_alpha - kl_beta
    
    def predict(self, X_test):
        # Add bias term
        X_test_aug = np.column_stack([np.ones(len(X_test)), X_test])
        
        # Predictive mean
        y_mean = X_test_aug @ self.mu_w
        
        # Predictive variance
        E_beta = self.a_beta / self.b_beta
        y_var = 1/E_beta + np.sum((X_test_aug @ self.Sigma_w) * X_test_aug, axis=1)
        
        return y_mean, np.sqrt(y_var)

# Example usage
np.random.seed(42)
n_train = 50
X_train = np.random.uniform(-2, 2, n_train).reshape(-1, 1)
y_train = 0.5 * X_train.ravel() + 0.3 * X_train.ravel()**2 + 0.1 * np.random.normal(0, 1, n_train)

X_test = np.linspace(-3, 3, 100).reshape(-1, 1)

# Fit variational model
vb_model = VariationalLinearRegression()
vb_model.fit(X_train, y_train)

y_pred, y_std = vb_model.predict(X_test)

print(f"Variational Inference converged: {vb_model.converged}")
print(f"Posterior weight mean: {vb_model.mu_w}")
print(f"Posterior weight std: {np.sqrt(np.diag(vb_model.Sigma_w))}")
```

## Uncertainty Quantification in Deep Learning

Modern deep learning incorporates uncertainty through various techniques.

### Monte Carlo Dropout

```python
# Conceptual implementation of MC Dropout for uncertainty quantification
def mc_dropout_predictions(model, X, n_samples=100, dropout_rate=0.5):
    """
    Simulate Monte Carlo Dropout for uncertainty quantification
    Note: This is a conceptual implementation
    """
    predictions = []
    
    for _ in range(n_samples):
        # In practice, you would enable dropout during inference
        # Here we simulate the effect with random masking
        
        # Simulate dropout by randomly masking features
        mask = np.random.binomial(1, 1-dropout_rate, X.shape[1])
        X_dropped = X * mask / (1-dropout_rate)  # Scale to maintain expected value
        
        # Make prediction (simplified linear model for demonstration)
        pred = X_dropped @ np.random.normal(0, 1, X.shape[1])
        predictions.append(pred)
    
    predictions = np.array(predictions)
    
    # Calculate statistics
    mean_pred = np.mean(predictions, axis=0)
    std_pred = np.std(predictions, axis=0)
    
    return mean_pred, std_pred, predictions

# Example usage
np.random.seed(42)
X_test = np.random.normal(0, 1, (100, 10))

mean_pred, std_pred, all_preds = mc_dropout_predictions(None, X_test, n_samples=50)

print("Monte Carlo Dropout Results:")
print(f"Mean prediction range: [{np.min(mean_pred):.3f}, {np.max(mean_pred):.3f}]")
print(f"Uncertainty range: [{np.min(std_pred):.3f}, {np.max(std_pred):.3f}]")

# Visualization
plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.plot(mean_pred, 'b-', alpha=0.7)
plt.fill_between(range(len(mean_pred)), 
                 mean_pred - 2*std_pred, 
                 mean_pred + 2*std_pred, 
                 alpha=0.3)
plt.title('MC Dropout Predictions')
plt.xlabel('Sample Index')
plt.ylabel('Prediction')
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 2)
plt.plot(std_pred, 'r-', alpha=0.7)
plt.title('Prediction Uncertainty')
plt.xlabel('Sample Index')
plt.ylabel('Standard Deviation')
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 3)
plt.hist(std_pred, bins=20, alpha=0.7)
plt.title('Uncertainty Distribution')
plt.xlabel('Standard Deviation')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Stochastic Processes

Stochastic processes model systems that evolve randomly over time.

### Markov Chains

```python
class MarkovChain:
    def __init__(self, transition_matrix, state_names=None):
        self.P = np.array(transition_matrix)
        self.n_states = len(self.P)
        self.state_names = state_names or [f"State {i}" for i in range(self.n_states)]
        
        # Validate transition matrix
        if not np.allclose(np.sum(self.P, axis=1), 1):
            raise ValueError("Rows of transition matrix must sum to 1")
    
    def simulate(self, n_steps, initial_state=0):
        """Simulate a path from the Markov chain"""
        states = [initial_state]
        current_state = initial_state
        
        for _ in range(n_steps):
            # Sample next state based on transition probabilities
            next_state = np.random.choice(self.n_states, p=self.P[current_state])
            states.append(next_state)
            current_state = next_state
        
        return states
    
    def steady_state(self):
        """Compute steady-state distribution"""
        # Find left eigenvector with eigenvalue 1
        eigenvals, eigenvecs = np.linalg.eig(self.P.T)
        
        # Find eigenvector corresponding to eigenvalue 1
        steady_idx = np.argmin(np.abs(eigenvals - 1))
        steady_vec = np.real(eigenvecs[:, steady_idx])
        
        # Normalize to get probability distribution
        steady_state = steady_vec / np.sum(steady_vec)
        
        return steady_state
    
    def absorption_probabilities(self, absorbing_states):
        """Calculate absorption probabilities for absorbing Markov chain"""
        n = self.n_states
        absorbing = np.array(absorbing_states)
        transient = np.array([i for i in range(n) if i not in absorbing_states])
        
        if len(transient) == 0:
            return np.eye(n)
        
        # Reorder matrix: transient states first, then absorbing
        Q = self.P[np.ix_(transient, transient)]  # Transient to transient
        R = self.P[np.ix_(transient, absorbing)]  # Transient to absorbing
        
        # Fundamental matrix N = (I - Q)^(-1)
        I = np.eye(len(transient))
        N = np.linalg.inv(I - Q)
        
        # Absorption probabilities B = N * R
        B = N @ R
        
        return B, N

# Example: Weather model
weather_transition = [
    [0.7, 0.2, 0.1],  # Sunny -> [Sunny, Cloudy, Rainy]
    [0.3, 0.4, 0.3],  # Cloudy -> [Sunny, Cloudy, Rainy]
    [0.2, 0.3, 0.5]   # Rainy -> [Sunny, Cloudy, Rainy]
]

weather_names = ['Sunny', 'Cloudy', 'Rainy']
weather_mc = MarkovChain(weather_transition, weather_names)

# Simulate weather
np.random.seed(42)
weather_path = weather_mc.simulate(30, initial_state=0)  # Start sunny

print("Weather Markov Chain Analysis:")
print("Transition Matrix:")
for i, row in enumerate(weather_transition):
    print(f"{weather_names[i]}: {row}")

# Steady state
steady = weather_mc.steady_state()
print(f"\nSteady-state probabilities:")
for i, prob in enumerate(steady):
    print(f"{weather_names[i]}: {prob:.3f}")

# Simulate multiple paths
n_simulations = 1000
path_length = 100
state_counts = np.zeros((path_length + 1, 3))

for _ in range(n_simulations):
    path = weather_mc.simulate(path_length, initial_state=0)
    for t, state in enumerate(path):
        state_counts[t, state] += 1

# Convert to probabilities
state_probs = state_counts / n_simulations

# Visualization
plt.figure(figsize=(15, 5))

# Sample path
plt.subplot(1, 3, 1)
plt.plot(weather_path[:20], 'o-', markersize=8)
plt.yticks([0, 1, 2], weather_names)
plt.xlabel('Day')
plt.ylabel('Weather State')
plt.title('Sample Weather Path (20 days)')
plt.grid(True, alpha=0.3)

# Convergence to steady state
plt.subplot(1, 3, 2)
for i, name in enumerate(weather_names):
    plt.plot(state_probs[:50, i], label=name, linewidth=2)
    plt.axhline(steady[i], color=plt.gca().lines[-1].get_color(), 
                linestyle='--', alpha=0.7)

plt.xlabel('Time')
plt.ylabel('Probability')
plt.title('Convergence to Steady State')
plt.legend()
plt.grid(True, alpha=0.3)

# Transition matrix heatmap
plt.subplot(1, 3, 3)
plt.imshow(weather_transition, cmap='Blues')
plt.colorbar()
plt.xticks(range(3), weather_names)
plt.yticks(range(3), weather_names)
plt.xlabel('To State')
plt.ylabel('From State')
plt.title('Transition Matrix')

for i in range(3):
    for j in range(3):
        plt.text(j, i, f'{weather_transition[i][j]:.1f}', 
                ha='center', va='center', fontweight='bold')

plt.tight_layout()
plt.show()
```

### Brownian Motion and Stochastic Differential Equations

```python
def simulate_brownian_motion(T, n_steps, n_paths=1):
    """Simulate Brownian motion paths"""
    dt = T / n_steps
    t = np.linspace(0, T, n_steps + 1)
    
    # Generate random increments
    dW = np.random.normal(0, np.sqrt(dt), (n_paths, n_steps))
    
    # Cumulative sum to get Brownian motion
    W = np.zeros((n_paths, n_steps + 1))
    W[:, 1:] = np.cumsum(dW, axis=1)
    
    return t, W

def simulate_geometric_brownian_motion(S0, mu, sigma, T, n_steps, n_paths=1):
    """Simulate Geometric Brownian Motion (Black-Scholes model)"""
    dt = T / n_steps
    t = np.linspace(0, T, n_steps + 1)
    
    # Generate random increments
    dW = np.random.normal(0, np.sqrt(dt), (n_paths, n_steps))
    
    # Simulate GBM using exact solution
    S = np.zeros((n_paths, n_steps + 1))
    S[:, 0] = S0
    
    for i in range(1, n_steps + 1):
        S[:, i] = S[:, i-1] * np.exp((mu - 0.5*sigma**2)*dt + sigma*dW[:, i-1])
    
    return t, S

# Simulate processes
np.random.seed(42)
T = 1.0  # Time horizon
n_steps = 1000
n_paths = 5

# Standard Brownian motion
t, W = simulate_brownian_motion(T, n_steps, n_paths)

# Geometric Brownian motion (stock price model)
S0 = 100  # Initial stock price
mu = 0.05  # Drift
sigma = 0.2  # Volatility
t_gbm, S = simulate_geometric_brownian_motion(S0, mu, sigma, T, n_steps, n_paths)

print("Stochastic Process Simulation:")
print(f"Time horizon: {T} years")
print(f"Number of steps: {n_steps}")
print(f"Number of paths: {n_paths}")
print(f"\nBrownian Motion final values: {W[:, -1]}")
print(f"GBM final values: {S[:, -1]}")

# Theoretical properties
print(f"\nTheoretical properties:")
print(f"BM variance at T={T}: {T:.3f}")
print(f"BM empirical variance: {np.var(W[:, -1]):.3f}")
print(f"GBM expected final value: {S0 * np.exp(mu * T):.2f}")
print(f"GBM empirical mean: {np.mean(S[:, -1]):.2f}")

# Visualization
plt.figure(figsize=(15, 10))

# Brownian motion paths
plt.subplot(2, 3, 1)
for i in range(n_paths):
    plt.plot(t, W[i], alpha=0.7, linewidth=1.5)
plt.xlabel('Time')
plt.ylabel('W(t)')
plt.title('Brownian Motion Paths')
plt.grid(True, alpha=0.3)

# GBM paths
plt.subplot(2, 3, 2)
for i in range(n_paths):
    plt.plot(t_gbm, S[i], alpha=0.7, linewidth=1.5)
plt.xlabel('Time')
plt.ylabel('S(t)')
plt.title('Geometric Brownian Motion')
plt.grid(True, alpha=0.3)

# Distribution of final values
plt.subplot(2, 3, 3)
# Simulate many paths for distribution
t_many, W_many = simulate_brownian_motion(T, n_steps, 1000)
plt.hist(W_many[:, -1], bins=50, alpha=0.7, density=True, label='Empirical')

# Theoretical normal distribution
x = np.linspace(W_many[:, -1].min(), W_many[:, -1].max(), 100)
theoretical = (1/np.sqrt(2*np.pi*T)) * np.exp(-x**2/(2*T))
plt.plot(x, theoretical, 'r-', linewidth=2, label='Theoretical N(0,T)')

plt.xlabel('W(T)')
plt.ylabel('Density')
plt.title(f'Distribution of W({T})')
plt.legend()
plt.grid(True, alpha=0.3)

# Quadratic variation
plt.subplot(2, 3, 4)
# Calculate quadratic variation for one path
dt = T / n_steps
quad_var = np.cumsum(np.diff(W[0])**2)
theoretical_qv = np.arange(1, n_steps + 1) * dt

plt.plot(t[1:], quad_var, label='Empirical QV', linewidth=2)
plt.plot(t[1:], theoretical_qv, 'r--', label='Theoretical QV = t', linewidth=2)
plt.xlabel('Time')
plt.ylabel('Quadratic Variation')
plt.title('Quadratic Variation of Brownian Motion')
plt.legend()
plt.grid(True, alpha=0.3)

# GBM log returns
plt.subplot(2, 3, 5)
log_returns = np.diff(np.log(S[0]))
plt.hist(log_returns, bins=50, alpha=0.7, density=True, label='Empirical')

# Theoretical normal distribution for log returns
x_ret = np.linspace(log_returns.min(), log_returns.max(), 100)
theoretical_ret = (1/np.sqrt(2*np.pi*sigma**2*dt)) * np.exp(-(x_ret - (mu-0.5*sigma**2)*dt)**2/(2*sigma**2*dt))
plt.plot(x_ret, theoretical_ret, 'r-', linewidth=2, label='Theoretical')

plt.xlabel('Log Return')
plt.ylabel('Density')
plt.title('Distribution of Log Returns')
plt.legend()
plt.grid(True, alpha=0.3)

# Volatility estimation
plt.subplot(2, 3, 6)
# Rolling volatility estimation
window = 50
rolling_vol = []
for i in range(window, len(log_returns)):
    vol = np.std(log_returns[i-window:i]) / np.sqrt(dt)
    rolling_vol.append(vol)

plt.plot(rolling_vol, alpha=0.7, linewidth=2)
plt.axhline(sigma, color='red', linestyle='--', linewidth=2, label=f'True Ïƒ = {sigma}')
plt.xlabel('Time Window')
plt.ylabel('Estimated Volatility')
plt.title('Rolling Volatility Estimation')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Key Takeaways

Advanced probability applications continue to evolve with computational capabilities:

**Probabilistic ML**: Gaussian Processes and variational inference provide principled uncertainty quantification in machine learning.

**Deep Learning**: Monte Carlo Dropout and Bayesian neural networks bring uncertainty to deep models.

**Stochastic Processes**: Markov chains and Brownian motion model complex temporal dependencies.

**Emerging Trends**: Probabilistic programming languages, neural ODEs, and quantum probability represent the cutting edge.

<Callout variant="tip">
**Future Directions**

- **Probabilistic Programming**: Languages like PyMC, Stan, and Edward democratize Bayesian modeling
- **Quantum Computing**: Quantum probability and quantum machine learning
- **Causal Inference**: Moving beyond correlation to causation using probabilistic methods
- **Federated Learning**: Distributed probabilistic inference across multiple parties
- **Interpretable AI**: Using probability to explain and understand AI decisions
</Callout>

<Pager 
  prev={{ title: "Real-World Applications", href: "/applications" }}
  next={{ title: "Best Practices", href: "/best-practices" }}
/>

</Prose>

<OnThisPage />
